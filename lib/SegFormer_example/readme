## First Time Setup (only do this once)
# 1. Import Docker Image
enroot import docker://nvcr.io#nvidia/pytorch:23.02-py3
# 2. Create .sqsh file
enroot create --name nvidia_pytorch_23.02 nvidia+pytorch+23.02-py3.sqsh
# 3. Start container with interactive job and install requirements
salloc -p dev_gpu_4_a100 -N 1 --ntasks-per-node=64 -t 30 --mem=510000 --gres=gpu:4

enroot start --root --rw -m pytorch-masterArbeit/:/workspace nvidia_pytorch_23.02 bash

pip install -r requirements.txt


## How to run jobs on bwUniCluster:
# 1. "nvidia_pytorch_23.02 nvidia+pytorch+23.02-py3.sqsh" needs to be in your home directory on bwUniCluster.

# 2. Create job.sh script in your home directory on bwUniCluster:
#!/bin/sh
enroot start --root --rw -m pytorch-masterArbeit/:/workspace nvidia_pytorch_23.02 python src/train.py --run=Train_Server --epochs 200 --backbone "b2"   # example for training 100 epochs with the b2 backbone
enroot start --root --rw -m pytorch-masterArbeit/:/workspace nvidia_pytorch_23.02 python src/train.py --run=Train_Server
# 4. Queue batch job on gpu_4_a100 node on bwUniCluster:
sbatch -p gpu_4_h100 -N 1 -t 48 --gres=gpu:4 job.sh   # example for training on 4 gpus for 8 hours (you can also choose a different gpu node - use "sinfo_t_idle" to check available nodes).

## other useful commands:
# 1. check status of job:
squeue
squeue --start  # shows estimated start time of queued jobs

# 2. cancel job:
scancel <job_id>

# 3. check available nodes:
sinfo_t_idle