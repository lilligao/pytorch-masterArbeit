## First Time Setup (only do this once)
# 1. Import Docker Image
enroot import docker://nvcr.io#nvidia/pytorch:23.02-py3
# 2. Create .sqsh file
enroot create --name nvidia_pytorch_23.02 nvidia+pytorch+23.02-py3.sqsh
# 3. Start container with interactive job and install requirements
salloc -p dev_gpu_4_a100 -N 1 --ntasks-per-node=64 -t 30 --mem=510000 --gres=gpu:4

enroot start --root --rw -m pytorch-masterArbeit/:/workspace nvidia_pytorch_23.02 bash

pip install -r requirements.txt


## How to run jobs on bwUniCluster:
# 1. "nvidia_pytorch_23.02 nvidia+pytorch+23.02-py3.sqsh" needs to be in your home directory on bwUniCluster.

# 2. Create job.sh script in your home directory on bwUniCluster:
#!/bin/sh
enroot start --root --rw -m pytorch-masterArbeit/:/workspace nvidia_pytorch_23.02 python src/train.py --run=Train_Server --epochs 200 --backbone "b2"   # example for training 100 epochs with the b2 backbone
enroot start --root --rw -m pytorch-masterArbeit/:/workspace nvidia_pytorch_23.02 python src/train.py --project='Masterarbeit Segformer Train' --run=Attempt1
# 4. Queue batch job on gpu_4_a100 node on bwUniCluster:
sbatch -p gpu_4_h100 -N 1 -t 48:00:00 --gres=gpu:4 job.sh # example for training on 4 gpus for 8 hours (you can also choose a different gpu node - use "sinfo_t_idle" to check available nodes).

## other useful commands:
# 1. check status of job:
squeue
squeue --start  # shows estimated start time of queued jobs

# 2. cancel job:
scancel <job_id>

# 3. check available nodes:
sinfo_t_idle

# job.sh

# Create a workspace to store the archive
ws_allocate data-ssd 60
# Create the archive from a local dataset folder (example)
tar -cvzf $(ws_find data-ssd)/dataset.tgz dataset/


#!/bin/sh
# Extract compressed input dataset on local SSD
tar -C $TMPDIR/ -xvzf $(ws_find data-ssd)/dataset.tgz
#cp -r ~/pytorch-masterArbeit/src  $TMPDIR/pytorch-masterArbeit/src
# start job
#enroot start --root --rw -m $TMPDIR/pytorch-masterArbeit/:/workspace nvidia_pytorch_23.02 python src/train.py --project='Masterarbeit Segformer Train' --run=Attempt2 --epochs=120 #--root=$TMPDIR/pytorch-masterArbeit/data/tless --checkpoints=$TMPDIR/results
# for train
enroot start --root --rw --mount=$HOME/pytorch-masterArbeit/:/workspace/ --mount=$TMPDIR/data/tless:/workspace/data/tless nvidia_pytorch_23.02 python src/train.py --project='Masterarbeit Segformer Train' --run=Attempt6_backbone_b1 --epochs=151 --backbone=b1
--val_split='test_primesense' # test dataset for validation
--train_split='train_pbr;train_primesense;train_render_reconst' --val_split='train_pbr;train_primesense;train_render_reconst'
# for test
enroot start --root --rw --mount=$HOME/pytorch-masterArbeit/:/workspace/ --mount=$TMPDIR/data/tless:/workspace/data/tless nvidia_pytorch_23.02 python src/test.py --load_checkpoints='./checkpoints/Attempt5_31Classes/epoch=150*.ckpt' --project='Masterarbeit Segformer Train' --run=TestOfAttempt6

enroot start --root --rw --mount=$HOME/pytorch-masterArbeit/:/workspace/ --mount=$HOME/data/tless:/workspace/data/tless nvidia_pytorch_23.02 python src/test.py --load_checkpoints='./checkpoints/Attempt5_31Classes/epochepoch=180-val_loss=0.05-val_iou=0.93.ckpt' --project='Masterarbeit Segformer Train' --run=TestOfAttempt5

# Before job completes save results on a workspace
#rsync -av $TMPDIR/pytorch-masterArbeit/checkpoints $(ws_find data-ssd)/checkpoints-${SLURM_JOB_ID}/

# copy checkpoints from server
scp -r uenhk@bwunicluster.scc.kit.edu:~/pytorch-masterArbeit/checkpoints ./checkpoints
